{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Data Science 101 - Lead Scoring Scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Problem\n",
    "\n",
    "Today we are going to look at a very common data science problem found in e-commerce and web based software or services (like Xero!). **Lead scoring!**\n",
    "\n",
    "### The Sales Process\n",
    "\n",
    "An education company named X Education sells online courses to industry professionals. On any given day, many professionals who are interested in the courses land on their website and browse for courses.\n",
    "\n",
    "1. Individuals land on the website, they might browse the courses, watch some vidoes and if they are interested they will fill up a form for the course. \n",
    "\n",
    "1. Once individuals fill out a form, they have now become a potential customer or **lead**\n",
    "\n",
    "1. Employees from the sales team will contact leads by emailing or calling them with the goal of getting the lead to sign up for the course - called a **conversion** \n",
    "\n",
    "### Situation\n",
    "\n",
    "- Currently only 30% of leads contacted go on to become a paying customer. \n",
    "- The CEO wants to grow the customer base by 20% over the next year \n",
    "\n",
    "### Complication \n",
    "\n",
    "- The head of the sales & advertising department has budget to increase advertising by 20% generate more leads, but they would not have enough reasource to still call every lead that signs up\n",
    "\n",
    "### Question\n",
    "\n",
    "- The department head knows that conversion rate of 30% means a lot of time and money is wasted on customers who never intent to sign up. \n",
    "- They have asked you if there is a way to call only the 'hottest' leads and get a better return on the amount of sales calls they make?\n",
    "\n",
    "### Answer \n",
    "\n",
    "- You tell the department head it is possible to use historical data from previous leads who have and have not become customers to build a model to score how likely a lead its to convert!\n",
    "\n",
    "Your idea for a lead scoring gets approved - now you need to look at the data you and decide what model you are going to build!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Tools\n",
    "\n",
    "1. Python\n",
    "1. Jupyter notebook\n",
    "1. Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing packages \n",
    "\n",
    "The data scientists tool kit in python ususally requires the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# package for multi-dimensional arrays and matrices\n",
    "import numpy as np\n",
    "\n",
    "# package for data manipulation and analysis\n",
    "import pandas as pd\n",
    "\n",
    "# packages for creating plots and graphs \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# visulaisation\n",
    "from matplotlib.pyplot import xticks\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data into a pandas dataframe\n",
    "leads_df = pd.read_csv('Leads - Leads.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class of the object\n",
    "type(leads_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many rows and columns \n",
    "leads_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leads_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leads_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leads_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leads_df.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leads_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leads_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking duplicates\n",
    "sum(leads_df.duplicated(subset = 'Prospect ID')) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting columns\n",
    "leads_df.Specialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leads_df['Specialization']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are \"select\" values in columns, that doesn't make any sense for occupation\n",
    "# This is because customer did not select any option from the list, hence it shows select.\n",
    "\n",
    "# Change 'Select' values to NaN.\n",
    "leads_df = leads_df.replace('Select', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leads_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(100*(leads_df.isnull().sum()/len(leads_df.index)), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets look at the specialization column\n",
    "leads_df['Specialization'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It maybe the case that lead has not entered any specialization if their option is not availabe on the list\n",
    "# We can make a category \"Others\" for missing values. \n",
    "leads_df['Specialization'] = leads_df['Specialization'].replace(np.nan, 'Other')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Data Exploration\n",
    "\n",
    "- `Prospect ID` is the identifer variable for the data across each row\n",
    "\n",
    "- `converted`  is our **target** variable (also known as the responce variable) i.e. the variable that we are interested in predicting. \n",
    "\n",
    "    - It is binary and the event either happens or it doesn't i.e. 0 or 1\n",
    "\n",
    "- We have a mix of 7 catagorical and continuous variables we can as **predictor** variables (also called explanitory variable, feature, input variable or independent variable)\n",
    "\n",
    "We can explore the data to see if there are any correlations between the target and predictor variables.\n",
    "\n",
    "First for our categorical variables lets look at side by side bar plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lead Origin\n",
    "sns.countplot(x = \"LeadOrigin\", hue = \"Converted\", data = leads_df)\n",
    "xticks(rotation = 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lead source\n",
    "fig, axs = plt.subplots(figsize = (15,7.5))\n",
    "sns.countplot(x = \"LeadSource\", hue = \"Converted\", data = leads_df)\n",
    "xticks(rotation = 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specialization\n",
    "fig, axs = plt.subplots(figsize = (15,7.5))\n",
    "sns.countplot(x = \"Specialization\", hue = \"Converted\", data = leads_df)\n",
    "xticks(rotation = 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Occupation\n",
    "fig, axs = plt.subplots(figsize = (15,7.5))\n",
    "sns.countplot(x = \"Occupation\", hue = \"Converted\", data = leads_df)\n",
    "xticks(rotation = 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our continuous variables we can look at box plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total Visits\n",
    "sns.boxplot(y = 'TotalVisits', x = 'Converted', data = leads_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total Time Spent on Website\n",
    "sns.boxplot(y = 'TotalTime', x = 'Converted', data = leads_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Page Views Per Visit\n",
    "sns.boxplot(y = 'PageViews', x = 'Converted', data = leads_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Model Building \n",
    "\n",
    "As we said above we have a binary target that we want to predict and a mix of 7 catagorical and continuous variables we can as predictor variables \n",
    "\n",
    "Because our target is binary (0 or 1) this is a **classification** problem - meaning that we want to predict weather a lead will convert or not. \n",
    "\n",
    "So we need to pick a model for classification. The first choice for many binary classification problems is a **logistic regression** model because of it's simplicity and interpretability. \n",
    "\n",
    "There are other options like **random forests** or **nueral networks** that can provide better accuracy, but depending on the data logistic regression can perform very well! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traning-testing split\n",
    "\n",
    "The next part of building a preditive model is to split the data into a **training** set and **testing** set. \n",
    "This lets us withold data from the model when traning so that we can test it's performance on data it has not seen before - just like it will be doing for our lead scoring. \n",
    "\n",
    "- Test 80% \n",
    "- Train 20% \n",
    "\n",
    "We also need to transform the data to get it ready for modelling \n",
    "- Remove the ID lable (not useful as a predictor)\n",
    "- Split out the target column\n",
    "- For categorical variables with multiple levels, create dummy features (one-hot encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# package for training models \n",
    "from sklearn import datasets, linear_model, metrics\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = leads_df['Converted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy1 = pd.get_dummies(leads_df[['LeadOrigin', 'LeadSource', 'Specialization', 'Occupation']], drop_first=True)\n",
    "dummy1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the results to the master dataframe\n",
    "X = pd.concat([leads_df[['TotalVisits', 'TotalTime', 'PageViews']], dummy1], axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and testing vars\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a model \n",
    "lm = linear_model.LogisticRegression()\n",
    "model = lm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in sample accuracy \n",
    "metrics.accuracy_score(y_train, model.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions on testing dataset\n",
    "y_test_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix \n",
    "confusion = metrics.confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the overall accuracy.\n",
    "metrics.accuracy_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = confusion[1,1] # true positive \n",
    "TN = confusion[0,0] # true negatives\n",
    "FP = confusion[0,1] # false positives\n",
    "FN = confusion[1,0] # false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the sensitivity of our logistic regression model\n",
    "se = TP / float(TP+FN)\n",
    "# Let us calculate specificity\n",
    "sp = TN / float(TN+FP)\n",
    "# Calculate false postive rate - predicting non conversion when customer has not converted\n",
    "fpr = FP/ float(TN+FP)\n",
    "# positive predictive value \n",
    "ppv = TP / float(TP+FP)\n",
    "# Negative predictive value\n",
    "npv = TN / float(TN+ FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Lift\n",
    "\n",
    "The great thing about logistic regression is it gives us the predicted probabilities of that observation falling into that class (in our case converion = 0 or conversion = 1). \n",
    "\n",
    "We can use this to rank our leads into deciles of the 'hottest' and compare the number of actual conversions in each decile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting predicted probabilities \n",
    "probs = pd.DataFrame(model.predict_proba(X_test), columns = ['prob_nc', 'prob_c'])\n",
    "\n",
    "# joining actual outcomes\n",
    "probs['actual'] = y_test.reset_index(drop=True)\n",
    "\n",
    "probs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cut data into deciles\n",
    "d = np.linspace(0.1,1,10).round(1)\n",
    "probs['deciles'] = pd.qcut(probs.prob_c, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# existing conversion rate\n",
    "x = sum(lift['sum'])/sum(lift['count'])\n",
    "\n",
    "# compare actual conversion in each decile\n",
    "lift = probs.groupby(probs.deciles)['actual'].agg([\"sum\", \"count\"]).reset_index()\n",
    "\n",
    "# calculate conversion probs\n",
    "lift['prob_con'] = lift['sum']/lift['count']\n",
    "\n",
    "# get cumulative counts and probabilities \n",
    "lift['sum_c'] = lift['sum'].iloc[::-1].cumsum()\n",
    "lift['prop_c'] = lift['sum_c']/731\n",
    "\n",
    "# old model - 40% conversion by random selection\n",
    "lift['old'] = lift['count']*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "\n",
    "ax.bar(lift.index, lift['sum'].iloc[::-1])\n",
    "ax.plot(lift.index, lift['old'].iloc[::-1], color = 'red')\n",
    "plt.xticks(lift.index, labels=lift.index+1)\n",
    "plt.title(\"Waterfall Analysis\")\n",
    "plt.xlabel(\"Deciles\")\n",
    "plt.ylabel(\"Conversions\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "\n",
    "ax.plot(lift.index, lift['prop_c'].iloc[::-1]*100, marker='o')\n",
    "ax.plot(lift.index, d*100, color = 'red', marker='o')\n",
    "plt.xticks(lift.index, labels=d*100)\n",
    "plt.title(\"Lift Chart\")\n",
    "plt.xlabel(\"% of Leads\")\n",
    "plt.ylabel(\"% of Conversions\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
